for (x_index in 1:length(x_grid)){
diff_matrix_allx[[x_index]][j_index, j_prime_index] =abs(D_j_prime_star[x_index] - D_j_star[x_index])
sigma_j_x = matrix_sigma_all_j_all_x[j_index, x_index]
sigma_j_prime_x = matrix_sigma_all_j_all_x[j_prime_index, x_index]
sigma_j_j_prime_x = matrix_sigma_all_j_j2_all_x[[x_index]][j_index, j_prime_index]
sigma_val_x = sigma_j_x + sigma_j_prime_x - 2*sigma_j_j_prime_x
if (sigma_val_x !=0){ #attention on a des sigma = 0 (bizarre) et donc pb
matrix_ratio_allx[[x_index]][j_index, j_prime_index] = diff_matrix_allx[[x_index]][j_index,j_prime_index]/sqrt(sigma_val_x)
}}
vec_max <- rep(0, length(x_grid))
for (x_index in 1:length(x_grid)){
vec_max[x_index] = max(matrix_ratio_allx[[x_index]])
}
return(max(vec_max))}}
compute_J_hat <- function(theta_star, I_hat, x_grid, degree, Z, matrix_gamma, matrix_sigma_all_j_all_x, matrix_sigma_all_j_j2_all_x){
j_index = 1
J = I_hat[j_index]
ratio = calcul_ratio(J, j_index, I_hat, x_grid, degree, Z, matrix_gamma, matrix_sigma_all_j_all_x, matrix_sigma_all_j_j2_all_x)
while (ratio > 10*theta_star){
j_index = j_index + 1
J = I_hat[j_index]
ratio = calcul_ratio(J, j_index, I_hat, x_grid,degree, Z, matrix_gamma, matrix_sigma_all_j_all_x, matrix_sigma_all_j_j2_all_x)
}
return(J)
}
calcul_ratio <- function(J, J_index, I_hat, x_grid, degree, Z, matrix_gamma, matrix_sigma_all_j_all_x, matrix_sigma_all_j_j2_all_x){
length_I_hat = length(I_hat)
res_mat <- matrix(0, length_I_hat, length(x_grid))
P_j_on_x <- create_dyadic_P_splines(x_grid, Z, J, degree) #on crée la base pour J donné
if (J>1){
g_hat_J_x <- P_j_on_x%*%matrix_gamma[J_index,][1:J]
}else{
g_hat_J_x <- matrix_gamma[J_index,][1:J]*P_j_on_x}
sigma_J_vect <- matrix_sigma_all_j_all_x[J_index,]
for (J_prime_index in J_index:length(I_hat)){
J_prime = I_hat[J_prime_index]
P_j_prime_on_x <- create_dyadic_P_splines(x_grid, Z, J_prime, degree)
if (J>1){
g_hat_J_prime_x <- P_j_prime_on_x%*%matrix_gamma[J_prime_index,][1:J_prime]
}else{
g_hat_J_prime_x <- matrix_gamma[J_prime_index,][1:J_prime]*P_j_prime_on_x}
sigma_J_prime_vect <- matrix_sigma_all_j_all_x[J_prime_index,]
for (x_index in 1:length(x_grid)){
sigma_J_J_prime <- abs(matrix_sigma_all_j_all_x[J_index,x_index]^{2} + matrix_sigma_all_j_all_x[J_prime_index,x_index]^{2} - 2*matrix_sigma_all_j_j2_all_x[[x_index]][J_index, J_prime_index])
denom = sqrt(sigma_J_J_prime)
if (denom!=0){
res_mat[J_prime_index, x_index] = abs(g_hat_J_prime_x[x_index] - g_hat_J_x[x_index])/(denom)
}}
}
return(max(res_mat))
}
valid_dim <- function(J, degree){
if (J-degree+1 <= 1){
return(FALSE)
}
else{
l = log(J - degree +1)/log(2)
if (as.integer(l)!=l){
return(FALSE)
}else{return(TRUE)}}}
#tests
#for (n in 1:10){
#  print(n)
#  simul <- simulate_data_3(c(400, 0.5, 0.9), g_sim_3, 2)
#  J_test <- compute_J_max(simul$W, simul$Z, simul$Y, 3)
#  print(J_test)
#  J_test <- 18
#  gamma_test <- estimation_gamma(J_test, simul$W, simul$Z, simul$Y, 3)}
lepski_bootstrap <- function(n_boot,valid_dim,x_grid, W, Z, Y, degree){#attention : ici x_grid juste pour calculer les sup norm
n = length(Z)
J_max = compute_J_max(W, Z, Y, degree)
print(J_max)
#J_max = 18
I_hat = seq(as.integer(0.1 * (log(J_max)^2))+1, as.integer(J_max), by = 1) #on commence au moins à 2 parce que 1 c'est pas une bonne solution
I_hat = sort(I_hat[sapply(I_hat,valid_dim, degree)]) #select only the valid dimensions
length_I_hat = length(I_hat)
# estimate the models (ie the gamma_J and M_boot for all J)
matrix_gamma = matrix(0, nrow = length_I_hat, ncol = max(I_hat))
list_M_boot <- list()
for (j_index in 1:length_I_hat){
j = I_hat[j_index]
print(j)
M_boot_j <- compute_M_bootstrap(j, W, Z, Y, degree)
list_M_boot[[j_index]] <- M_boot_j
gamma_J <- M_boot_j%*%Y
gamma_J_zeros <- c(gamma_J, rep(0, max(I_hat) - j)) #add zeros for good dimension
matrix_gamma[j_index,] = gamma_J_zeros #ATTENTION ! prendre que les J premiers termes dans les prochains algos pour pas prendre les zéros!!!
}
#draw the w_i
W_boot_matrix <- matrix(rnorm(n_boot * n, mean = 0, sd = 1), nrow = n_boot, ncol = n)
#compute the sigma_J(x) for all values of J and all values of x
matrix_sigma_all_j_all_x <- matrix(0, nrow = length_I_hat, ncol = length(x_grid))
matrix_all_u <- matrix(0, nrow = length_I_hat, ncol = n) #n number of data points (ie length(Z))
for (j_index in 1:length_I_hat){
j = I_hat[j_index]
h_hat_j_on_Z = calcul_hat_g_on_Z(j, Z, matrix_gamma[j_index,1:j], a, b, Z, degree)
u_hat_j = Y - h_hat_j_on_Z
matrix_all_u[j_index,] = u_hat_j
P_j_on_x <- create_dyadic_P_splines(x_grid, Z, j, degree) #create for all x_grid
U_j_j <- create_matrix_U(u_hat_j, u_hat_j)
middle_matrix <- list_M_boot[[j_index]]%*%U_j_j%*%t(list_M_boot[[j_index]]) #ok
vec_sigma_j <- rep(0, length(x_grid))
if (length(middle_matrix)>1){
for (x_index in 1:length(x_grid)){
vec_sigma_j[x_index] <- P_j_on_x[x_index,]%*%(middle_matrix)%*%P_j_on_x[x_index,]
}
}
else{
for (x_index in 1:length(x_grid)){
vec_sigma_j[x_index] = middle_matrix*P_j_on_x[x_index]^{2}
}
}
matrix_sigma_all_j_all_x[j_index,] = vec_sigma_j #ligne j prend les valeurs des sigma_j(x) pour toutes les valeurs de x_grid
}
#compute the sigma_J,J_2(x) for all values of x
matrix_sigma_all_j_j2_all_x <- replicate(length(x_grid), matrix(0, length_I_hat, length_I_hat), simplify = FALSE)
for (j_index in 1:length_I_hat){
j = I_hat[j_index]
P_j_on_x <- create_dyadic_P_splines(x_grid, Z, j, degree)
for (j_prime_index in (j_index):length_I_hat){
j_prime = I_hat[j_prime_index]
#calcul de sigma_j,jprime(x) pour tous les x_grid et ensuite on met dans les matrices
U_j_jp <- create_matrix_U(matrix_all_u[j_index,], matrix_all_u[j_prime_index,])
P_j_prime_on_x <- create_dyadic_P_splines(x_grid, Z,j_prime,degree)
middle_matrix <- list_M_boot[[j_index]]%*%U_j_jp%*%t(list_M_boot[[j_prime_index]])
if (j == 1 && j_prime == 1){
vec_sigma_j_jprime <- rep(0, length(x_grid))
for (x_index in 1:length(x_grid)){
vec_sigma_j_jprime[x_index] = middle_matrix*P_j_on_x[x_index]*P_j_prime_on_x[x_index]
matrix_sigma_all_j_j2_all_x[[x_index]][j_index, j_prime_index] = vec_sigma_j_jprime[x_index]
}
}
if (j == 1 && j_prime >1){
vec_sigma_j_jprime <- rep(0, length(x_grid))
for (x_index in 1:length(x_grid)){
vec_sigma_j_jprime[x_index] = P_j_on_x[x_index]*(middle_matrix%*%P_j_prime_on_x[x_index,])
matrix_sigma_all_j_j2_all_x[[x_index]][j_index, j_prime_index] = vec_sigma_j_jprime[x_index]
}
}
if (j>1 && j_prime == 1){
vec_sigma_j_jprime <- rep(0, length(x_grid))
for (x_index in 1:length(x_grid)){
vec_sigma_j_jprime[x_index] = (P_j_on_x[x_index,]%*%middle_matrix)*P_j_prime_on_x[x_index]
matrix_sigma_all_j_j2_all_x[[x_index]][j_index, j_prime_index] = vec_sigma_j_jprime[x_index]
}
}
if (j>1 && j_prime >1){
vec_sigma_j_jprime <- rep(0, length(x_grid))
for (x_index in 1:length(x_grid)){
vec_sigma_j_jprime[x_index] = P_j_on_x[x_index,]%*%(middle_matrix)%*%P_j_prime_on_x[x_index,]
matrix_sigma_all_j_j2_all_x[[x_index]][j_index, j_prime_index] = vec_sigma_j_jprime[x_index]
}
}
}}
#compute the T stat for each draw of w
T_i = rep(0, n_boot)
for (i in 1:n_boot){
T_i[i] = T_stat_D(x_grid, Y, Z, W, degree, I_hat, W_boot_matrix[i,], list_M_boot, matrix_sigma_all_j_all_x, matrix_all_u, matrix_sigma_all_j_j2_all_x)
}
alpha = min(0.5, 1/J_max)
theta = quantile(T_i, probs = 1 - alpha)
J_n_hat = I_hat[length(I_hat)-1] #on prend l'avant dernier pour être le plus petit strict
J_hat = compute_J_hat(theta,I_hat, x_grid, degree, Z, matrix_gamma, matrix_sigma_all_j_all_x, matrix_sigma_all_j_j2_all_x)
J_tilde = min(J_hat, J_n_hat)
return(J_tilde)
}
#x = seq(min(Z), max(Z), by = 0.1)
#lepski_bootstrap(100,valid_dim,x, W, Z, Y,3) #ok fonctionne avec splines
#### Lepski simpler version ####
compute_J_max <- function(W, Z, Y, degree){
J_init = degree - 1 + 2
J_next = degree - 1 + 2^{2}
n = length(Z)
s_1 = 1/calcul_s_J(J_init, W, Z, Y, degree)
s_2 = 1/calcul_s_J(J_next, W, Z, Y, degree)
prev_ratio = s_1/sqrt(n)
new_ratio = s_2/sqrt(n)
J = J_next
s_J_hat = s_2
if ((prev_ratio <= 10 && new_ratio > 10 && s_J_hat !=999)) {
} else {
while (!(prev_ratio <= 10 && new_ratio > 10 && s_J_hat != 999)){
J = J + 1
# Look for a valid J
while (!valid_dim(J, degree)) {
J = J + 1}
# Calculate s_hat_J and update ratios
s_hat_J = calcul_s_J(J, W, Z, Y, degree)
prev_ratio = new_ratio
new_ratio = s_hat_J / sqrt(n)
}
return(J)}}
lepski_chen <- function(c_0,W,Z,Y,degree, valid_dim){
n = length(Z)
x_grid = seq(min(Z), max(Z), length.out = 10*n)
J_max = compute_J_max(W, Z, Y, degree)
#J_max = 50
I_hat = seq(as.integer(0.1 * (log(J_max)^2))+1, as.integer(J_max), by = 1) #on commence au moins à 2 parce que 1 c'est pas une bonne solution
I_hat = sort(I_hat[sapply(I_hat,valid_dim, degree)]) #select only the valid dimensions
length_I_hat = length(I_hat)
J_index = 1
J = I_hat[J_index]
condition = FALSE
while ((J_index<length(I_hat) && condition == FALSE)){
gamma_J <- estimation_gamma(J,W,Z,Y, degree)
s_J <- calcul_s_J(J, W, Z, Y, degree)
V_J = V_hat_J(J, n, s_J)
J_prime_index = J_index + 1
J_prime = I_hat[J_prime_index]
while ((J_prime_index < length(I_hat) && condition == FALSE)){
gamma_J_prime <- estimation_gamma(J_prime, W, Z, Y, degree)
s_J_prime <- calcul_s_J(J_prime, W, Z, Y, degree)
V_J_prime = V_hat_J(J, n, s_J_prime)
a = difference_norm(gamma_J, gamma_J_prime, J, J_prime, Z, degree, x_grid)
b = c_0*(V_J + V_J_prime)
if(a<b){
condition = TRUE}
else{
condition = FALSE}
J_prime_index = J_prime_index + 1
J_prime = I_hat[J_prime_index]}
J_index = J_index + 1
J = I_hat[J_index]
}
J_opt = J
return(J_opt)
}
V_hat_J <- function(J, n, s_J_hat){
return (sqrt(log(n)/n)/s_J_hat) #the one in our case
}
difference_norm <- function(gamma_J, gamma_J_prime, J, J_prime, Z, degree, x_grid){
P_x_grid_J <- create_dyadic_P_splines(x_grid, Z, J,degree)
P_x_grid_J_prime <- create_dyadic_P_splines(x_grid, Z, J_prime,degree)
g_hat_J_x_grid <- P_x_grid_J%*%gamma_J
g_hat_J_prime_x_grid <- P_x_grid_J_prime%*%gamma_J_prime
m_m = max(Z) - min(Z)
n = length(Z)
return(m_m*sum((g_hat_J_prime_x_grid - g_hat_J_x_grid)^{2})/n)
}
#lepski_chen(1, simul$W,simul$Z,simul$Y,3, valid_dim )
#### Monte Carlo d'abord sans sélection du paramètre J ####
#on choisit des valeurs de J d'abord et on regarde pour un nombre de samples ce qu'on obtient comme MSE par ex sur un grid
MC_fixed_J <- function(J, n_MC, degree, x_evaluation, g_0, case, data_param){
list_gamma <- list() #liste des gamma obtenus
list_g_hat_on_x <- list() #liste de l'estimation de g sur la grille
list_W <- list()
list_Y <- list()
list_Z <- list()
for (n in 1:n_MC){
#generate data
simul <- simulate_data_3(data_param, g_0, case)
W <- simul$W
Y <- simul$Y
Z <- simul$Z
list_W[[n]] <- W
list_Y[[n]] <- Y
list_Z[[n]] <- Z
#compute g_hat_J
gamma_hat_J <- estimation_gamma(J, W, Z, Y, degree)
list_gamma[[n]] <- gamma_hat_J
#compute the function on the grid
basis <- create_dyadic_P_splines(x_evaluation, Z, J, degree)
g_hat_on_x <- basis%*%gamma_hat_J
list_g_hat_on_x[[n]] <- g_hat_on_x
}
g_0_on_x <- g_0(x_evaluation, case)
return(list(list_gamma = list_gamma, list_g_hat_on_x = list_g_hat_on_x,
list_W = list_W, list_Y = list_Y, list_Z = list_Z, g_0_on_x = g_0_on_x))
}
#test <- MC_fixed_J(10, 100, 3, seq(-2, 2, by = 0.1), g_sim_3, 2, c(1000, 0.5, 0.9))
compute_perf <- function(res_MC, measure){
n_MC = length(res_MC$list_gamma)
MSE = 0
var = 0
bias = 0
sup_norm = 0
M = 0
n_val <- length(res_MC$list_g_hat_on_x[[1]])
if (measure == 'MSE'){
for (i in 1:n_MC){
MSE = MSE + sum((res_MC$list_g_hat_on_x[[i]] - res_MC$g_0_on_x)^{2})
}
MSE = MSE/(n_MC*n_val)
return(MSE)
}
if (measure == 'Var'){
avg <- rep(0, n_val)
for (x in 1:n_val){
for (n in 1:n_MC){
avg[x] <- avg[x] + res_MC$list_g_hat_on_x[[n]][x]/n_MC
}}
for (n in 1:n_MC){
var = var + sum((res_MC$list_g_hat_on_x[[n]] - avg)^{2})
}
var = var/(n_MC*n_val)
return(var)
}
if (measure == 'bias'){
n_val <- length(res_MC$list_g_hat_on_x[[1]])
avg <- rep(0, n_val)
for (x in 1:n_val){
for (n in 1:n_MC){
avg[x] <- avg[x] + res_MC$list_g_hat_on_x[[n]][x]
}}
for (n in 1:n_MC){
bias = bias + sum((res_MC$g_0_on_x - avg/n_MC)^{2})
}
bias = bias/(n_MC*n_val)
return(bias)
}
if (measure == 'supnorm'){
sup_norm_vect <- rep(0, n_MC)
for (n in 1:n_MC){
sup_norm_vect[n] = max(abs(res_MC$list_g_hat_on_x[[n]] - res_MC$g_0_on_x))
}
sup_norm = mean(sup_norm_vect)
return(sup_norm)
}
if (measure == 'M'){
M_vect <- rep(0, n_MC)
for (n in 1:n_MC){
Omega <- create_W(res_MC$list_W[[n]])
M_vect[n] = calcul_M_g_hat_test_sample(res_MC$list_g_hat_on_x[[n]], Omega, n_val, res_MC$list_Y[[n]])
}
return (mean(M_vect))
}
}
#compute_perf(test, 'M') #calcul du critère M moyen
#compute_perf(test, 'supnorm') #supnorm
#compute_perf(test, 'Var')
#compute_perf(test, 'MSE')
#compute_perf(test, 'bias')
# ok on a bien MSE = var + bias -> à faire : créer grille avec résultats
library(ggplot2)
plot_mean_true <- function(res_MC, x_evaluation,J, degree, rhozw, rhouv, case){
n_MC = length(res_MC$list_gamma)
n_val = length(res_MC$list_g_hat_on_x[[1]])
#compute the avg
avg <- rep(0, n_val)
for (x in 1:n_val){
for (n in 1:n_MC){
avg[x] <- avg[x] + res_MC$list_g_hat_on_x[[n]][x]/n_MC
}}
data <- data.frame(x_evaluation, avg, g_0_on_x = res_MC$g_0_on_x)
graph <- ggplot(data, aes(x = x_evaluation)) +
geom_line(aes(y = g_0_on_x, colour = 'True Function')) + # Changed the label
geom_line(aes(y = avg, colour = 'Estimate by MC')) + # Changed the label
ylab("Function value") +
xlab("Evaluation points") +
scale_colour_manual(
name = "Functions", # Change this to your desired legend title
values = c("True Function" = "blue", "Estimate by MC" = "red") # Customize colors
) +
ggtitle(paste("True vs avg, (n_MC =", n_MC, ", n_val =", n_val, ", rhowz =", rhozw, ", rhouv =", rhouv, ", degree =", degree, ", case = ", case, ", J =", J, ")"))
return(graph)
}
#plot_mean_true(test, seq(-2, 2, by = 0.1), 10)
plot_allcurves_true <- function(res_MC, x_evaluation,J, degree, rhozw, rhouv, case){
n_MC = length(res_MC$list_gamma)
data_allcurves <- do.call(rbind, lapply(1:n_MC, function(i) data.frame(
x = x_evaluation,
y = res_MC$list_g_hat_on_x[[i]],
id = i
)))
true_values <- data.frame(x = x_evaluation, y = res_MC$g_0_on_x)
n_val = length(res_MC$list_g_hat_on_x[[1]])
#compute the avg
avg <- rep(0, n_val)
for (x in 1:n_val){
for (n in 1:n_MC){
avg[x] <- avg[x] + res_MC$list_g_hat_on_x[[n]][x]/n_MC
}}
data_avg <- data.frame(x = x_evaluation, y = avg)
ggplot() +
geom_line(data = data_allcurves, aes(x = x, y = y, group = id,  color = "Estimated functions"), alpha = 0.5) +  # Estimations
geom_line(data = true_values, aes(x = x, y = y, color = "True Function"), size = 1.2) +    # True function
geom_line(data = data_avg, aes(x = x, y = y, color = "Average Estimate"), size = 1) +
scale_color_manual(name = "Legend",
values = c("Estimated functions" = "grey", "True Function" = "green", "Average Estimate" = "black")) +
theme_minimal() +
labs(x = "Grid for estimation",
y = "Estimated values of g")+
ggtitle(paste("MC results, (n_MC =", n_MC, ", n_val =", n_val, ", rhowz =", rhozw, ", rhouv =", rhouv, ", degree =", degree, ", case = ", case, ", J =", J, ")"))
}
#### Monte Carlo avec sélection de J ####
MC_CV <- function(method, n_MC, vect_J_to_test, p_train, degree, x_grid, g_0, case, data_param){
list_W <- list()
list_Y <- list()
list_Z <- list()
list_J_opt <- rep(0, n_MC)
list_gamma_opt <- list()
list_g_hat_on_x <- list()
for (n in 1:n_MC){
#generate data
simul <- simulate_data_3(data_param, g_0, case)
W <- simul$W
Y <- simul$Y
Z <- simul$Z
list_W[[n]] <- W
list_Y[[n]] <- Y
list_Z[[n]] <- Z
#compute everything
if (method == 'CV_M'){
res <- optimization_CV_M(Z, W, Y, vect_J_to_test, p_train, degree, x_grid)}
if (method == 'CV_MSE'){
res <- optimization_CV_MSE(Z, W, Y, vect_J_to_test, p_train, degree, x_grid)
}
#compute the optimal J
J_opt <- res$J_opt
list_J_opt[n] <- J_opt
#compute the gamma_J_opt
gamma_hat_J_opt <- res$gamma_hat_j_opt
list_gamma_opt[[n]] <- gamma_hat_J_opt
#compute the function on the grid
g_hat_on_x <- res$g_hat_on_x_opt
list_g_hat_on_x[[n]] <- g_hat_on_x
}
g_0_on_x <- g_0(x_grid, case)
return(list(list_J_opt = list_J_opt, list_gamma = list_gamma_opt,
list_g_hat_on_x = list_g_hat_on_x, g_0_on_x = g_0_on_x,
list_W = list_W, list_Y = list_Y, list_Z = list_Z))
}
#test <- MC_CV('CV_MSE',100, c(4, 6, 10), 0.8, 3, seq(-2, 2, by = 0.1), g_sim_3, 2, c(200, 0.5, 0.9))
# fonctionne pas tjrs : pb, voir comment gérer ça pour continuer les simus
MC_lepski_boot <- function(n_MC, n_boot, x_eval, valid_dim, degree, g_0, case, data_param){
list_W <- list()
list_Y <- list()
list_Z <- list()
list_J_opt <- rep(0, n_MC)
list_gamma_opt <- list()
list_g_hat_on_x <- list()
for (n in 1:n_MC){
#generate data
simul <- simulate_data_3(data_param, g_0, case)
W <- simul$W
Y <- simul$Y
Z <- simul$Z
list_W[[n]] <- W
list_Y[[n]] <- Y
list_Z[[n]] <- Z
#compute the optimal J
x_grid = seq(min(Y), max(Y), length.out = 200)
J_opt <- lepski_bootstrap(n_boot, valid_dim, x_grid, W, Z, Y, degree)
list_J_opt[n] <- J_opt
#compute the gamma_J_opt
gamma_hat_J_opt <- estimation_gamma(J_opt, W, Z, Y, degree)
list_gamma_opt[[n]] <- gamma_hat_J_opt
#compute the function on the grid
basis <- create_dyadic_P_splines(x_eval, Z, J_opt, degree)
g_hat_on_x <- basis%*%gamma_hat_J_opt
list_g_hat_on_x[[n]] <- g_hat_on_x
}
g_0_on_x <- g_0(x_eval, case)
return(list(list_J_opt = list_J_opt, list_gamma = list_gamma_opt,
list_g_hat_on_x = list_g_hat_on_x, g_0_on_x = g_0_on_x,
list_W = list_W, list_Y = list_Y, list_Z = list_Z))
}
#test <- MC_lepski_boot(2, 10, valid_dim, seq(-2, 2, by = 0.1), 3, g_sim_3, 2, c(200, 0.5, 0.9))
MC_lepski <-  function(n_MC, x_eval, degree, valid_dim, g_0, case, data_param){
c_0 = 10
list_W <- list()
list_Y <- list()
list_Z <- list()
list_J_opt <- rep(0, n_MC)
list_gamma_opt <- list()
list_g_hat_on_x <- list()
for (n in 1:n_MC){
#generate data
simul <- simulate_data_3(data_param, g_0, case)
W <- simul$W
Y <- simul$Y
Z <- simul$Z
list_W[[n]] <- W
list_Y[[n]] <- Y
list_Z[[n]] <- Z
#compute the optimal J
J_opt <- lepski_chen(c_0, W, Z, Y, degree, valid_dim)
list_J_opt[n] <- J_opt
#compute the gamma_J_opt
gamma_hat_J_opt <- estimation_gamma(J_opt, W, Z, Y, degree)
list_gamma_opt[[n]] <- gamma_hat_J_opt
#compute the function on the grid
basis <- create_dyadic_P_splines(x_eval, Z, J_opt, degree)
g_hat_on_x <- basis%*%gamma_hat_J_opt
list_g_hat_on_x[[n]] <- g_hat_on_x
}
g_0_on_x <- g_0(x_eval, case)
return(list(list_J_opt = list_J_opt, list_gamma = list_gamma_opt,
list_g_hat_on_x = list_g_hat_on_x, g_0_on_x = g_0_on_x,
list_W = list_W, list_Y = list_Y, list_Z = list_Z))
}
MC_selection <- function(method, n_MC, vect_J_to_test, p_train, degree, x_grid, n_boot, valid_dim, g_0, case, data_param, bool_plot){ #x_grid pour évaluer
if (method == 'CV_M' || method == 'CV_MSE'){
MC_CV_res = MC_CV(method, n_MC, vect_J_to_test, p_train, degree, x_grid, g_0, case, data_param)
if (bool_plot == 1){
plot(plot_mean_true(MC_CV_res, seq(-2, 2, by = 0.1), method))}
return(MC_CV_res)
}
if (method == 'lepski_boot'){
MC_lepski_boot_res = MC_lepski_boot(n_MC, n_boot, valid_dim, x_grid, degree, g_0, case, data_param)
if (bool_plot == 1){
plot(plot_mean_true(MC_lepski_boot_res, seq(-2, 2, by = 0.1), method))}
return(MC_lepski_boot_res)
}
if (method == 'lepski'){
MC_lepski_res = MC_lepski(n_MC, x_grid, degree, valid_dim, g_0, case, data_param)
if (bool_plot == 1){
plot(plot_mean_true(MC_lepski_res, seq(-2, 2, by = 0.1), method))}
return(MC_lepski_res)
}
}
#MC_selection('lepski_boot', 10, c(4, 6, 10), 0.5, 3, seq(-2, 2, by=0.1), 1, valid_dim, g_sim_3, 2, c(200, 0.5, 0.9), 1)
